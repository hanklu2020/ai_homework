{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_0.62%_Hao-Bang_Lu.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TfuBG6djZK2m"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF8DeF0__yz6",
        "colab_type": "text"
      },
      "source": [
        "**Feel free check our [Youtube Link](https://www.youtube.com/watch?v=8g9U342bfrU&feature=youtu.be)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eje_B1eEAXbq",
        "colab_type": "text"
      },
      "source": [
        "# Create and Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfuBG6djZK2m",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_ZyaSyjJPPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 15\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "my_model_path = 'my_model.h5' # .h5 or.hdf5"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FunLFYJxUtoj",
        "colab_type": "text"
      },
      "source": [
        "**Neuron network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OixyYFQh5UJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "3372f370-2e15-42fd-dbc6-685379fac29a"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(kernel_size=3, filters=12, use_bias=False, padding='same', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "  tf.keras.layers.Activation('relu'),\n",
        "  \n",
        "  tf.keras.layers.Conv2D(kernel_size=6, filters=24, use_bias=False, padding='same', strides=2),\n",
        "  tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "  tf.keras.layers.Activation('relu'),\n",
        "  \n",
        "  tf.keras.layers.Conv2D(kernel_size=6, filters=32, use_bias=False, padding='same', strides=2),\n",
        "  tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "  tf.keras.layers.Activation('relu'),\n",
        "  ## ↑ convolution ↑ #### ↓ fully-connected layer ↓ ##\n",
        "  tf.keras.layers.Flatten(),\n",
        "  \n",
        "  tf.keras.layers.Dense(200, use_bias=False),\n",
        "  tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "  tf.keras.layers.Activation('relu'),\n",
        "  \n",
        "  tf.keras.layers.Dropout(0.3), # To solve overfitting\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "print(model.output_shape)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',  # or 'sgd', tf.keras.optimizers.Adam() or tf.train.AdamOptimizer()\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 10)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 12)        36        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 24)        10368     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 24)        72        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 14, 14, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 32)          27648     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 32)          96        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1568)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               313600    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 200)               600       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2010      \n",
            "=================================================================\n",
            "Total params: 354,538\n",
            "Trainable params: 354,002\n",
            "Non-trainable params: 536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2otbiVaNU58G",
        "colab_type": "text"
      },
      "source": [
        "**Import datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ckAp3RLKDtO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0b819ae3-5df9-4f66-acd0-d1340d314f53"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(training_data, training_labels),(validation_data, validation_labels) = mnist.load_data()\n",
        "\n",
        "image_size=28\n",
        "\n",
        "training_data = training_data.reshape(training_data.shape[0], image_size, image_size, 1).astype('float32') # .astype('float32')\n",
        "validation_data = validation_data.reshape(validation_data.shape[0], image_size, image_size, 1).astype('float32')             # .astype('float32')\n",
        "\n",
        "training_data = tf.keras.utils.normalize(training_data, axis=1) # training_data, validation_data = training_data/255, validation_data/255\n",
        "validation_data = tf.keras.utils.normalize(validation_data, axis=1)\n",
        "\n",
        "print(training_data.dtype)\n",
        "print(training_data.shape)\n",
        "print(validation_data.dtype)\n",
        "print(validation_data.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float32\n",
            "(60000, 28, 28, 1)\n",
            "float32\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw-hOWlHcNIb",
        "colab_type": "text"
      },
      "source": [
        "## Training & evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUGC_6_9m81_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d0c8142-a0dc-4cff-da6c-fdf6608bdb32"
      },
      "source": [
        "import math\n",
        "\n",
        "# steps_per_epoch & validation_steps\n",
        "compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / BATCH_SIZE))    # BATCH_SIZE = 100\n",
        "StepsPerEpoch = compute_steps_per_epoch(training_data.shape[0])            # training_data.shape[0] = 60000 (TRAINING SIZE)\n",
        "valSteps = compute_steps_per_epoch(validation_data.shape[0])                     # validation_data.shape[0] = 10000 (VALIDATION SIZE)\n",
        "print(\"Steps per epoch: \", StepsPerEpoch)\n",
        "\n",
        "# callbacks 1 - define the checkpoint\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "# callbacks 2 - define the lr decay\n",
        "def lr_decay(epoch):\n",
        "  return 0.01 * math.pow(0.6, epoch)\n",
        "lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\n",
        "\n",
        "# model fit\n",
        "model.fit(training_data, training_labels, steps_per_epoch=StepsPerEpoch, epochs=EPOCHS, shuffle=True,\\\n",
        "          validation_data=(validation_data, validation_labels), validation_steps=valSteps, callbacks=[model_checkpoint, lr_decay_callback])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps per epoch:  600\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/15\n",
            "600/600 [==============================] - 80s 134ms/step - loss: 0.1260 - accuracy: 0.9617 - val_loss: 0.0641 - val_accuracy: 0.9796 - lr: 0.0100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.006.\n",
            "Epoch 2/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0442 - accuracy: 0.9860 - val_loss: 0.0324 - val_accuracy: 0.9895 - lr: 0.0060\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0036.\n",
            "Epoch 3/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0242 - accuracy: 0.9927 - val_loss: 0.0339 - val_accuracy: 0.9896 - lr: 0.0036\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0021599999999999996.\n",
            "Epoch 4/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0216 - val_accuracy: 0.9931 - lr: 0.0022\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001296.\n",
            "Epoch 5/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.0250 - val_accuracy: 0.9924 - lr: 0.0013\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0007775999999999998.\n",
            "Epoch 6/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0231 - val_accuracy: 0.9928 - lr: 7.7760e-04\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004665599999999999.\n",
            "Epoch 7/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0218 - val_accuracy: 0.9934 - lr: 4.6656e-04\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00027993599999999994.\n",
            "Epoch 8/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0211 - val_accuracy: 0.9934 - lr: 2.7994e-04\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00016796159999999993.\n",
            "Epoch 9/15\n",
            "600/600 [==============================] - 79s 132ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0213 - val_accuracy: 0.9940 - lr: 1.6796e-04\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00010077695999999997.\n",
            "Epoch 10/15\n",
            "600/600 [==============================] - 78s 131ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0215 - val_accuracy: 0.9934 - lr: 1.0078e-04\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 6.0466175999999974e-05.\n",
            "Epoch 11/15\n",
            "600/600 [==============================] - 80s 133ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0217 - val_accuracy: 0.9938 - lr: 6.0466e-05\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 3.627970559999999e-05.\n",
            "Epoch 12/15\n",
            "600/600 [==============================] - 81s 134ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0218 - val_accuracy: 0.9937 - lr: 3.6280e-05\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 2.1767823359999992e-05.\n",
            "Epoch 13/15\n",
            "600/600 [==============================] - 81s 135ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0220 - val_accuracy: 0.9938 - lr: 2.1768e-05\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 1.3060694015999994e-05.\n",
            "Epoch 14/15\n",
            "600/600 [==============================] - 81s 135ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0220 - val_accuracy: 0.9939 - lr: 1.3061e-05\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 7.836416409599996e-06.\n",
            "Epoch 15/15\n",
            "600/600 [==============================] - 81s 135ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0221 - val_accuracy: 0.9938 - lr: 7.8364e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8e44c45b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "194cOoZ8cXAU",
        "colab_type": "text"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwLhk78wlnzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(my_model_path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5YcbKcOAu4j",
        "colab_type": "text"
      },
      "source": [
        "# Reload Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh-eUjQmA6rN",
        "colab_type": "text"
      },
      "source": [
        "## initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5RuCL0BBA4K",
        "colab_type": "text"
      },
      "source": [
        "**load model**\n",
        "\n",
        "PS. Remember to upload \"my_model.h5\" file before start runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7I8JpqKA6DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 10\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "new_model_path = 'my_model.h5'\n",
        "new_Model = tf.keras.models.load_model(\n",
        "    new_model_path)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCL5ceyJBLIs",
        "colab_type": "text"
      },
      "source": [
        "**load datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ1TJC6CBKsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(training_data, training_labels),(validation_data, validation_labels) = mnist.load_data()\n",
        "\n",
        "image_size=28\n",
        "\n",
        "training_data = training_data.reshape(training_data.shape[0], image_size, image_size, 1).astype('float32') # .astype('float32')\n",
        "validation_data = validation_data.reshape(validation_data.shape[0], image_size, image_size, 1).astype('float32')             # .astype('float32')\n",
        "\n",
        "training_data = tf.keras.utils.normalize(training_data, axis=1) # training_data, validation_data = training_data/255, validation_data/255\n",
        "validation_data = tf.keras.utils.normalize(validation_data, axis=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-QlwupiBSe4",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGvLI96XBWOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da9ac3d9-efa9-46b5-f18c-ac2f5f118a7d"
      },
      "source": [
        "val_loss, val_acc = new_Model.evaluate(validation_data, validation_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0221 - accuracy: 0.9938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8kRf1cuBZNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pick one picture to validate\n",
        "print(\"Label:\" + str(validation_labels[0]))\n",
        "predictions = new_Model.predict(validation_data)\n",
        "import numpy as np\n",
        "print(\"Prediction\" + str(np.argmax(predictions[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyEx4KMmBXJY",
        "colab_type": "text"
      },
      "source": [
        "## Continue training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBoKUtbKBfE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training, evaluating\n",
        "\n",
        "import math\n",
        "\n",
        "# steps_per_epoch & validation_steps\n",
        "compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / BATCH_SIZE))    # BATCH_SIZE = 100\n",
        "StepsPerEpoch = compute_steps_per_epoch(training_data.shape[0])            # training_data.shape[0] = 60000 (TRAINING SIZE)\n",
        "valSteps = compute_steps_per_epoch(validation_data.shape[0])                     # validation_data.shape[0] = 10000 (VALIDATION SIZE)\n",
        "print(\"Steps per epoch: \", StepsPerEpoch)\n",
        "\n",
        "# callbacks 1 - define the checkpoint\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "# callbacks 2 - define the lr decay\n",
        "def lr_decay(epoch):\n",
        "  return 0.01 * math.pow(0.6, epoch)\n",
        "lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\n",
        "\n",
        "# new model fit\n",
        "new_Model.fit(training_data, training_labels, steps_per_epoch=StepsPerEpoch, epochs=EPOCHS, shuffle=True,\\\n",
        "          validation_data=(validation_data, validation_labels), validation_steps=valSteps, callbacks=[model_checkpoint, lr_decay_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWZfYU1gzWY",
        "colab_type": "text"
      },
      "source": [
        "**save new model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dTDDKUaf9Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_Model.save(new_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm50wngMB57w",
        "colab_type": "text"
      },
      "source": [
        "# Group 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skg0hMVpB_Iy",
        "colab_type": "text"
      },
      "source": [
        "authors: \n",
        "*   **Hao-Bang Lu** C24031211\n",
        "*   **Montenegro Aguilar Mauricio Adrian** F04077150"
      ]
    }
  ]
}